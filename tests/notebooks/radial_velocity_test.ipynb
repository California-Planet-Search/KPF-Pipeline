{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import warnings\n",
    "from astropy.coordinates import Angle\n",
    "from astropy.io import fits\n",
    "from astropy.modeling import models, fitting\n",
    "from astropy.time import Time\n",
    "import math\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import configparser\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from astropy import constants as const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.radial_velocity.src.alg import RadialVelocityAlg\n",
    "from modules.radial_velocity.src.alg_rv_init import RadialVelocityAlgInit\n",
    "load_dotenv()\n",
    "TEST_DIR = os.getenv('KPFPIPE_TEST_DATA') \n",
    "print('TEST_DIR:', TEST_DIR)\n",
    "FIT_G = fitting.LevMarLSQFitter()\n",
    "LIGHT_SPEED = 299792.458   # light speed in km/s\n",
    "class DotDict(dict):\n",
    "    pass\n",
    "MODULE_DIR = '../../modules/radial_velocity/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tests.regression import test_radial_velocity\n",
    "print(\"start test_rv_init_exception\")\n",
    "test_radial_velocity.test_rv_init_exception()\n",
    "print(\"test_rv_ccf_init_exception\")\n",
    "test_radial_velocity.test_rv_ccf_init_exception()\n",
    "#test_radial_velocity.test_compute_rv_by_cc_exception()\n",
    "#test_radial_velocity.test_neid_compute_rv_by_cc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MJD_TO_JD = 2400000.5\n",
    "class RadialVelocityStats:\n",
    "    \"\"\" This module defines class ' RadialVelocityStats' and methods to do statistic analysis on radial velocity\n",
    "    results. (this is currently for radial velocity development and testing only).\n",
    "\n",
    "    Attributes:\n",
    "         rv_result_set (list): A container storing radial velocity result from fits of level 1 data.\n",
    "         total_set (int): Total elements in `rv_result_set`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_rv_results: list = None):\n",
    "        self.rv_result_set = list() if obs_rv_results is None else obs_rv_results.copy()\n",
    "        self.total_set = 0 if obs_rv_results is None else len(obs_rv_results)\n",
    "\n",
    "    def get_collection(self):\n",
    "        return self.rv_result_set, self.total_set\n",
    "\n",
    "    def add_data(self, ccf_rv: float, obj_jd: float):\n",
    "        self.rv_result_set.append({'jd': obj_jd, 'mean_rv': ccf_rv})\n",
    "        self.total_set = len(self.rv_result_set)\n",
    "        return self.rv_result_set, self.total_set\n",
    "\n",
    "    def analyze_multiple_ccfs(self, ref_date=None):\n",
    "        \"\"\" Statistic analysis on radial velocity numbers of multiple observation resulted by `RadialVelocityAlg`.\n",
    "\n",
    "        Args:\n",
    "            ref_date (str, optional): Reference time in the form Julian date format.  Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            dict: Analysis data.\n",
    "\n",
    "        \"\"\"\n",
    "        obs_rvs, total_obs = self.get_collection()\n",
    "        jd_list = np.array([obs_rv['jd'] for obs_rv in obs_rvs])\n",
    "        if ref_date is None:\n",
    "            ref_jd = self.get_start_day(jd_list)\n",
    "        else:\n",
    "            ref_jd = Time(ref_date, format='isot', scale='utc').jd\n",
    "        rv_stats = dict()\n",
    "        rv_stats['start_jd'] = ref_jd\n",
    "        rv_stats['hour'] = (jd_list-ref_jd) * 24.0\n",
    "        rv_stats['day'] = (jd_list-ref_jd)\n",
    "        rv_stats['values'] = np.array([obs_rv['mean_rv'] for obs_rv in obs_rvs])\n",
    "        rv_stats['mean'] = np.mean(rv_stats['values'])\n",
    "        rv_stats['sigma'] = np.std(rv_stats['values'] - rv_stats['mean'])\n",
    "\n",
    "        return rv_stats\n",
    "\n",
    "    @staticmethod\n",
    "    def get_start_day(jd_list: np.ndarray):\n",
    "        min_jd = np.amin(jd_list)\n",
    "        day_part = math.floor(min_jd - MJD_TO_JD)\n",
    "        return MJD_TO_JD+day_part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy of start_logger from logger.py\n",
    "def get_level(lvl:str) -> int:\n",
    "    if lvl == 'debug': return logging.DEBUG\n",
    "    elif lvl == 'info': return logging.INFO\n",
    "    elif lvl == 'warning': return logging.WARNING\n",
    "    elif lvl == 'error': return logging.ERROR\n",
    "    elif lvl == 'critical': return logging.CRITICAL\n",
    "    else: return logging.NOTSET\n",
    "\n",
    "def start_logger(logger_name: str, config: str):\n",
    "    if config is None: \n",
    "        # a config file is not provided, so don't start logger\n",
    "        print('[{}] missing log configuration...not starting a new logger'.format(\n",
    "            logger_name))\n",
    "        return None\n",
    "    config_obj = configparser.ConfigParser()\n",
    "    res = config_obj.read(config)\n",
    "    if res == []:\n",
    "        return None\n",
    "\n",
    "    log_cfg = config_obj['LOGGER']\n",
    "\n",
    "    log_start = log_cfg.get('start_log', False)\n",
    "    log_path = log_cfg.get('log_path', 'log')\n",
    "    log_lvl = log_cfg.get('log_level', logging.WARNING)\n",
    "    log_verbose = log_cfg.getboolean('log_verbose', True)\n",
    "    # logger.setLevel(get_level(log_lvl))\n",
    "        \n",
    "    # if log_start:\n",
    "    #     # setup a log format\n",
    "    #     formatter = logging.Formatter('[%(name)s][%(levelname)s]:%(message)s')\n",
    "    #     # setup a log file\n",
    "    #     f_handle = logging.FileHandler(log_path, mode='w') # logging to file\n",
    "    #     f_handle.setLevel(get_level(log_lvl))\n",
    "    #     f_handle.setFormatter(formatter)\n",
    "    #     logger.addHandler(f_handle)\n",
    "\n",
    "    #     if log_verbose: \n",
    "    #         # also print to terminal \n",
    "    #         s_handle = logging.StreamHandler()\n",
    "    #         s_handle.setLevel(get_level(log_lvl))\n",
    "    #         s_handle.setFormatter(formatter)\n",
    "    #         logger.addHandler(s_handle)\n",
    "    # return logger\n",
    "\n",
    "\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.setLevel(get_level(log_lvl))\n",
    "    logger.propagate = False\n",
    "\n",
    "    formatter = logging.Formatter('[%(name)s][%(levelname)s]:%(message)s')\n",
    "    s_handle = logging.StreamHandler()\n",
    "    s_handle.setLevel(get_level(log_lvl))\n",
    "    s_handle.setFormatter(formatter)\n",
    "    logger.addHandler(s_handle)\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gaussian_on_curve(g_curve, curve_x, curve_y, order=None, title=None, ref_curve=None, ref_gaussian=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    label_curve = \"rv on order \" + str(order) if order is not None else 'all orders '\n",
    "    plt.plot(curve_x, curve_y, 'ko', label=label_curve)\n",
    "    if ref_curve is not None:\n",
    "        plt.plot(curve_x, ref_curve, 'mo', label=\"neid_ccf\"  )\n",
    "    ref_label = '/'+str(\"{0:.6f}\".format(ref_gaussian.mean.value)) if ref_gaussian is not None else ''\n",
    "    plt.plot(curve_x, g_curve(curve_x), label='mean:'+str(\"{0:.6f}\".format(g_curve.mean.value))+ ref_label)\n",
    "                                                      \n",
    "    plt.legend(loc=\"lower right\", prop={'size': 12})\n",
    "    plt.title(title) if title is not None else None\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_ccf_to_csv(ccf, csvfile):\n",
    "    with open(csvfile, mode='w') as result_file:\n",
    "        result_writer = csv.writer(result_file)\n",
    "        for i in range(0, np.shape(ccf)[0]):\n",
    "            row_data = list()\n",
    "            row_data.append(i)\n",
    "            for val in ccf[i, :]:\n",
    "                row_data.append(val)\n",
    "            result_writer.writerow(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_two_csv(file1, file2):\n",
    "    file1_rows = list()\n",
    "    file2_rows = list()\n",
    "    \n",
    "    with open(file1) as csv1:\n",
    "        csvReader1 = csv.reader(csv1)\n",
    "        for row in csvReader1:\n",
    "            file1_rows.append(row)\n",
    "    with open(file2) as csv2:\n",
    "        csvReader2 = csv.reader(csv2)\n",
    "        for row in csvReader2:\n",
    "            file2_rows.append(row)\n",
    "   \n",
    "    f1 = np.array(file1_rows, dtype=float)\n",
    "    f2 = np.array(file2_rows, dtype=float)\n",
    "    \n",
    "    diff_index = np.where((f1-f2) != 0.0)    \n",
    "    \n",
    "    diff_at = list(zip(list(diff_index[0]), list(diff_index[1])))\n",
    "\n",
    "    return diff_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "dev = 'developed'\n",
    "ref = 'reference'\n",
    "\n",
    "def plot_velocity_time(rv_info, title, label, color, time_unit='hrs', savefig=None):\n",
    " \n",
    "    total_rv = np.size(rv_info[dev]['values'])\n",
    "\n",
    "    if time_unit == 'hrs':\n",
    "        rv_delta_time = rv_info[dev]['hour'] \n",
    "    else:\n",
    "        rv_delta_time = rv_info[dev]['day']\n",
    "        \n",
    "    plt.figure(figsize=(10,12))     \n",
    "    s = 100\n",
    "    ymax = -10000\n",
    "    ymin = 10000\n",
    "    for k in rv_info.keys():\n",
    "        rv_offset = (rv_info[k]['values'] - rv_info[k]['mean']) * 1000.0\n",
    "        ymax = max(np.amax(rv_offset), ymax)\n",
    "        ymin = min(np.amin(rv_offset), ymin)\n",
    "        rv_sigma = rv_info[k]['sigma']*1000.0\n",
    "        plt.scatter(rv_delta_time, rv_offset, s, c=color[k], edgecolors='b', \n",
    "                    label=label[k] + ' sigma = '+ \"{:0.6f}\".format(rv_sigma)+' m/s')\n",
    "        \n",
    "\n",
    "    plt.legend(loc=\"upper right\", prop={'size':12})\n",
    "    plt.xlabel('Times ['+time_unit+']')\n",
    "    plt.ylabel('RV [m/s]')\n",
    "\n",
    "    ymax = math.ceil(ymax)+1\n",
    "    ymin = math.floor(ymin)-1\n",
    "    xmin = math.ceil(np.amin(rv_delta_time))\n",
    "    xmax = math.floor(np.amax(rv_delta_time))\n",
    "    if time_unit == 'hrs':\n",
    "        plt.xlim((xmin-1, xmax+1))\n",
    "    else:\n",
    "        plt.xlim((xmin-20, xmax+20))\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    \n",
    "    if savefig is not None:\n",
    "        plt.savefig(savefig)\n",
    "    plt.show()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fits(ccf, mean, out_fits):\n",
    "    hdu = fits.PrimaryHDU(ccf)\n",
    "    hdu.header['Date'] = str(datetime.datetime.now())\n",
    "    if mean is not None: \n",
    "        hdu.header['CCF-RVC'] = (str(mean), ' Baryc RV (km/s)')\n",
    "    hdu.writeto(out_fits, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_test(target_data, data_result):\n",
    "    \"\"\"Check if 2D data is consistent with that from a reference fits.\n",
    "\n",
    "    Args:\n",
    "        target_data (numpy.ndarray): Array of data to compare to.\n",
    "        data_result (numpy.ndarray): Array of data to be checked.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        dict:  Comparison result between the data and the reference data, like::\n",
    "\n",
    "            {\n",
    "                'result': 'ok'              # if the data is consistent with the reference.\n",
    "            }\n",
    "            {\n",
    "                'result': 'error',          # if the data is not the same as the reference.\n",
    "                'msg': <reason message>\n",
    "            }\n",
    "    \"\"\"\n",
    "    t_y, t_x = np.shape(target_data)\n",
    "    r_y, r_x = np.shape(data_result)\n",
    "\n",
    "    if t_y != r_y or t_x != r_x:\n",
    "        return {'result': 'error', 'msg': 'dimension is not the same'}\n",
    "\n",
    "    not_nan_data_idx = np.argwhere(~np.isnan(data_result))\n",
    "    not_nan_target_idx = np.argwhere(~np.isnan(target_data))\n",
    "\n",
    "    if np.size(not_nan_data_idx) != np.size(not_nan_target_idx):\n",
    "        return {'result': 'error', 'msg': 'NaN data different'}\n",
    "    elif np.size(not_nan_data_idx) != 0:\n",
    "        if not (np.array_equal(not_nan_data_idx, not_nan_target_idx)):\n",
    "            return {'result': 'error', 'msg': 'NaN data different'}\n",
    "        else:\n",
    "            not_nan_target = target_data[~np.isnan(target_data)]\n",
    "            not_nan_data = data_result[~np.isnan(data_result)]\n",
    "            diff_idx = np.where(not_nan_target - not_nan_data)[0]\n",
    "\n",
    "            if diff_idx.size > 0:\n",
    "                diff_val = not_nan_target - not_nan_data\n",
    "                diff_max = np.amax(diff_val[diff_idx])\n",
    "                diff_max_rv = np.amax(data_result[r_y-1, :] - target_data[t_y - 1, :])\n",
    "                return {'result': 'error', 'msg': 'data is not the same at ' + str(diff_idx.size) +\n",
    "                                                    ' points and max difference of last row ' + str(diff_max_rv)}\n",
    "\n",
    "    return {'result': 'ok'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RV computation on NEID L1 data and results from Optimal Extraction module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "neid_L1_dir = TEST_DIR + '/NEIDdata/TAUCETI_20191217/L1/neidL1_20191217T'\n",
    "neid_L2_dir = TEST_DIR + '/NEIDdata/TAUCETI_20191217/L2/neidL2_20191217T'\n",
    "\n",
    "width_type = 'for_width_3'            # 'for_width_3', 'for_fixed_width', 'for_width_2'\n",
    "method = 'normal'      #'norect','vertical' or 'normal'\n",
    "\n",
    "# spectrum_L1_path = '../../modules/optimal_extraction/results/NEID_3sigma/' + width_type + '/NEID_*_extraction_'+method\n",
    "spectrum_L1_path = '../../test_results/neid/stacked_2fiber_flat_L0_neidTemp_2D20191217T'\n",
    "outfolder = MODULE_DIR+'results/NEID/'+width_type\n",
    "\n",
    "s_order=3\n",
    "e_order= 82\n",
    "# s_order = 20\n",
    "# e_order = 24\n",
    "order_diff = 7\n",
    "s_x_pos = 600\n",
    "\n",
    "# import pdb;pdb.set_trace()\n",
    "spectraname_op = glob.glob(spectrum_L1_path + '*_' + method +'_L1.fits')\n",
    "spectraname_neid = glob.glob(neid_L1_dir + '*.fits')\n",
    "spectraname_op.sort()\n",
    "\n",
    "print('<test>: \\n ', spectraname_op)\n",
    "total_file = len(spectraname_op)\n",
    "\n",
    "# code_list = [ f[f.index('NEID_0')+len('NEID_'):f.index('_extraction_optimal')] for f in spectraname_op]\n",
    "d_stamp = '2D20191217T'\n",
    "code_list = [f[f.index(d_stamp) + len(d_stamp):(f.index(method)-1)] for f in spectraname_op]\n",
    "print('<test>: \\n', 'total ', (len(code_list)), ' code in fits: ', code_list)\n",
    "\n",
    "config_file = MODULE_DIR + 'configs/default.cfg'\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file)\n",
    "\n",
    "logger = start_logger(\"OrderTraceAlg\", config_file)\n",
    "\n",
    "bc_time = 2458591.5 # 2458278.5 # 2458591.5   # None\n",
    "corr_path = '../../modules/barycentric_correction/results/'\n",
    "rv_init = RadialVelocityAlgInit(config, logger, bc_time=bc_time, bc_corr_path=corr_path)\n",
    "init_result = rv_init.start(print_debug='')\n",
    "\n",
    "\n",
    "rv_dev_info = RadialVelocityStats()\n",
    "rv_ref_info = RadialVelocityStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv('KPFPIPE_TEST_DATA') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare ratio table and reweighting result for unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make ratio table (from NEID L2)\n",
    "\n",
    "neid_L2_files = sorted(glob.glob(neid_L2_dir+'*.fits'))\n",
    "reweighting_method = 'ccf_max'\n",
    "allcpp = []\n",
    "for f in range(len(neid_L2_files)):\n",
    "    hdul = fits.open(neid_L2_files[f])\n",
    "    ccf = hdul[12].data           # a Table HDU\n",
    "    allcpp.append(np.nanpercentile(ccf[s_order+order_diff:e_order+order_diff+1, :], 95, axis=1))\n",
    "\n",
    "index = np.where(allcpp == np.max(allcpp))\n",
    "index = np.squeeze(index[0])\n",
    "print('index: ', index, neid_L2_files[index])\n",
    "template_file_hdulist = fits.open(neid_L2_files[index])\n",
    "template_ccf = (template_file_hdulist[12].data)[(s_order+order_diff):(e_order+order_diff+1), :]\n",
    "\n",
    "code_str = neid_L2_files[index][len(neid_L2_dir): ]\n",
    "pos = code_str.find('.fits')\n",
    "code_str = code_str[0:pos]\n",
    "print('code: ', code_str)\n",
    "\n",
    "output_csv = MODULE_DIR+'results/NEID/ccf_ratio_'+ code_str+'_'+str(s_order+order_diff) +'_'+str(e_order+order_diff)+'.csv'\n",
    "print(output_csv)\n",
    "rw_ratio_df = RadialVelocityAlg.make_reweighting_ratio_table(template_ccf, s_order+order_diff, \n",
    "                                    e_order+order_diff, reweighting_method, max_ratio = 1.0, \n",
    "                                    output_csv=output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# do reweighting (from NEID L2)\n",
    "\n",
    "ratio_df = pd.read_csv(output_csv)\n",
    "total_order = e_order-s_order+1\n",
    "\n",
    "rv_lev2_info = RadialVelocityStats()\n",
    "rv_lev2_reweight_info = RadialVelocityStats()\n",
    "velocities = init_result['data']['velocity_loop'] + 4.0\n",
    "\n",
    "for f in neid_L2_files:\n",
    "    print(f)\n",
    "    neid_L2_file_hdulist = fits.open(f)\n",
    "    rv_est = neid_L2_file_hdulist[0].header['QRV']\n",
    "    L1_time = neid_L2_file_hdulist[12].header['CCFJDSUM']\n",
    "    rv_guess = neid_L2_file_hdulist[12].header['CCFRVSUM']\n",
    "    ny, nx = np.shape(neid_L2_file_hdulist[12].data)\n",
    "    rv_lev2_info.add_data(rv_guess, L1_time)\n",
    "\n",
    "    neid_L2_ccf = neid_L2_file_hdulist[12].data[s_order+order_diff: , :]\n",
    "    old_L2_ccf = np.zeros((total_order+RadialVelocityAlg.ROWS_FOR_ANALYSIS, nx))\n",
    "    old_L2_ccf[0:total_order, :] = neid_L2_ccf[0:total_order, :]\n",
    "    old_L2_ccf[total_order+RadialVelocityAlg.ROWS_FOR_ANALYSIS-2, :] = velocities\n",
    "    old_L2_ccf[total_order+RadialVelocityAlg.ROWS_FOR_ANALYSIS-1, :] = np.sum(old_L2_ccf[1:total_order, :], axis=0)\n",
    "    before_ccf_fit, before_ccf_mean, before_x, before_y = RadialVelocityAlg.fit_ccf(old_L2_ccf[-1, :], rv_est,\n",
    "                                                                                    velocities)\n",
    "    reweighted_lev2_ccf = RadialVelocityAlg.reweight_ccf(neid_L2_ccf, total_order, ratio_df.values,\n",
    "                                                         reweighting_method, s_order = s_order+order_diff, \n",
    "                                                         do_analysis=True)\n",
    "    \n",
    "    rw_lev2_ccf_fit, rw_lev2_ccf_mean, n_x, n_y = RadialVelocityAlg.fit_ccf(reweighted_lev2_ccf[-1, :], rv_est, \n",
    "                                                                            velocities)\n",
    "    make_fits(reweighted_lev2_ccf, rw_lev2_ccf_mean, MODULE_DIR+'results/NEID/reweighted_ccf_'+str(s_order+order_diff) +'_'+str(e_order+order_diff)+'.fits')\n",
    "    rv_lev2_reweight_info.add_data(rw_lev2_ccf_mean, L1_time) \n",
    "    plot_gaussian_on_curve(rw_lev2_ccf_fit, n_x, n_y, ref_curve=before_y, ref_gaussian=before_ccf_fit)\n",
    "    break\n",
    "\n",
    "\"\"\"\n",
    "rv_before_stats = rv_lev2_info.analyze_multiple_ccfs()\n",
    "rv_after_stats = rv_lev2_reweight_info.analyze_multiple_ccfs()\n",
    "rv_stats = {ref: rv_before_stats, dev: rv_after_stats}\n",
    "plot_velocity_time(rv_stats, method, {dev: 'after reweighted', ref: 'before reweighted'}, {dev: 'cyan', ref: 'yellow'})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial Velocity computation with or without reweighting ccf orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get reweighting ratio table\n",
    "out_csv = TEST_DIR + '/radial_velocity_test/results/neid_tauceti_ratio_' + str(s_order) + '_' + str(e_order) + '.csv'\n",
    "ratio_df = pd.read_csv(out_csv)\n",
    "ratio_ref = ratio_df.values\n",
    "\n",
    "for f in range(total_file):\n",
    "#for f in range(1):\n",
    "    code = code_list[f]\n",
    "    spec_name = spectraname_op[f]\n",
    "    print('File ', f+1, 'of ', total_file, ' file: ', spec_name, ' code: ', code)\n",
    "    \n",
    "    L1_data = fits.open(spec_name)\n",
    "    spec_data = L1_data[1].data\n",
    "    neid_hdulist = fits.open(neid_L1_dir+code+'.fits')\n",
    "    print('<test> :','neid L1 file: ', neid_L1_dir+code)\n",
    "\n",
    "    neid_ref, ref_header = fits.getdata(neid_L2_dir+code+'.fits', header=True)   # ref is not used in this test\n",
    "    \n",
    "    order_diff = 7   # order difference between the result from optimal extaction module and NEID L1 data\n",
    "    h = neid_hdulist[0].header\n",
    "    wave = neid_hdulist[7].data[order_diff:, :]  # wavelength calibration \n",
    "    \n",
    "    if 'data' not in init_result:\n",
    "        continue\n",
    "    radial_vel = RadialVelocityAlg(spec_data, h, init_result, wave,  config, logger)\n",
    "    \n",
    "    ny, nx = np.shape(spec_data)\n",
    "    e_x_pos = nx - s_x_pos\n",
    "    \n",
    "    rv_guess = h['QRV'] if 'QRV' in h else init_result['data'][RadialVelocityAlgInit.RV_CONFIG][RadialVelocityAlgInit.START_RV]\n",
    "    cindy_rv  = radial_vel.compute_rv_by_cc(start_order = s_order, \n",
    "                                            end_order = e_order, \n",
    "                                            start_x = s_x_pos, \n",
    "                                            end_x = e_x_pos, print_progress='')\n",
    "                                            #  end_x = e_x_pos, ref_ccf=ratio_ref, print_progress='')\n",
    "    if not cindy_rv:\n",
    "        continue\n",
    "    cindy_ccf = cindy_rv['ccf_ary']\n",
    "    \n",
    "    total_order = radial_vel.spectrum_order\n",
    "    gaussian_fit, gaussian_mean, g_x, g_y = radial_vel.fit_ccf(\n",
    "                                    cindy_ccf[total_order+RadialVelocityAlg.ROWS_FOR_ANALYSIS-1, :], rv_guess, \n",
    "                                    cindy_ccf[total_order+RadialVelocityAlg.ROWS_FOR_ANALYSIS-2, :])\n",
    "                                        \n",
    "    \"\"\" gaussian on reweighted ccf orders\n",
    "    ref_fits = outfolder+'/rv_optimal_'+method+'_'+code+'.fits'\n",
    "    ref_hdulist = fits.open(ref_fits)\n",
    "    ref_data = ref_hdulist[0].data\n",
    "    ref_fit, ref_mean, ref_x, ref_y = radial_vel.fit_ccf(\n",
    "                                    ref_data[total_order+RadialVelocityAlg.ROWS_FOR_ANALYSIS-1, :], rv_guess, \n",
    "                                    ref_data[total_order+RadialVelocityAlg.ROWS_FOR_ANALYSIS-2, :])\n",
    "    plot_gaussian_on_curve(gaussian_fit, g_x, g_y, title=ref_fits, ref_curve=ref_y, ref_gaussian=ref_fit)\n",
    "    \"\"\"\n",
    "    \n",
    "    wave = neid_hdulist[7].data\n",
    "     \n",
    "    radial_vel_neid = RadialVelocityAlg(neid_hdulist[1].data,  h, init_result, wave, config, logger)  \n",
    "    # import pdb;pdb.set_trace()\n",
    "    neid_rv = radial_vel_neid.compute_rv_by_cc(start_order = s_order+order_diff, \n",
    "                                                end_order = e_order+order_diff, \n",
    "                                                start_x = s_x_pos,\n",
    "                                                end_x = e_x_pos,  print_progress='')\n",
    "\n",
    "    neid_ccf = neid_rv['ccf_ary']  \n",
    "    neid_fit, neid_mean, n_x, n_y = radial_vel.fit_ccf(\n",
    "                                    neid_ccf[total_order+RadialVelocityAlg.ROWS_FOR_ANALYSIS-1, :], rv_guess, \n",
    "                                    neid_ccf[total_order+RadialVelocityAlg.ROWS_FOR_ANALYSIS-2, :])\n",
    "    \n",
    "    # outfile_neid = outfolder + '/rv_neid_'+code+'.fits'\n",
    "    # outfile_cindy = outfolder + '/rv_cindy_'+width_type+'_'+code+'.fits'\n",
    "       \n",
    "    L1_time = cindy_rv['jd'] \n",
    "    plot_gaussian_on_curve(gaussian_fit, g_x, g_y, title=spectraname_op[f], ref_curve=n_y, ref_gaussian=neid_fit)\n",
    "    \n",
    "    # target_file_to_test is for output (make_fits) or comparison (reslt_test)\n",
    "    # target_file_to_test = '../results/NEID/'+width_type+'/rv_output/'+ 'rv_cindy_'+method+ '_'+code+'.fits'\n",
    "    # target_data = fits.getdata(target_file_to_test)\n",
    "    # target_file_to_test = '../results/NEID/'+width_type+'/rv_output/'+ 'rv_'+method+ '_' + code +  \\\n",
    "    #            '_' + str(s_order) + '_' + str(e_order) + '.fits'\n",
    "    # make_fits(cindy_ccf, gaussian_mean, outfolder+'/rv_optimal_'+method+'_'+code+'.fits')\n",
    "    \n",
    "    \"\"\"\n",
    "     # for norect only\n",
    "    target_file_to_test =  '/Users/cwang/documents/KPF/KPF-Pipeline/test_results/neid/tmp/'+ \\\n",
    "             'stacked_2fiber_flat_L0_neidTemp_2D20191217T' + code_list[f] + '_norect_L1_L2.fits'\n",
    "    \n",
    "    print('target_file: ', target_file_to_test)\n",
    "    target_hdu = fits.open(target_file_to_test)\n",
    "    df = pd.DataFrame(target_hdu[6].data)\n",
    "    target_data = df.values\n",
    "    \n",
    "    compare_result = result_test(target_data, cindy_ccf)\n",
    "    print(compare_result)\n",
    "    \"\"\"    \n",
    "    rv_dev_results, total_dev_rv = rv_dev_info.add_data(gaussian_fit.mean.value, L1_time)\n",
    "    rv_ref_results, total_ref_rv = rv_ref_info.add_data(neid_fit.mean.value, L1_time)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the template observation from above NEID ccf results\n",
    "lev2_result_files = sorted(glob.glob(outfolder+'/rv_optimal_'+method+'*.fits'))\n",
    "reweighting_method = 'ccf_max'\n",
    "allcpp = []\n",
    "total_order = e_order-s_order+1\n",
    "for f in range(len(lev2_result_files)):\n",
    "    hdul = fits.open(lev2_result_files[f])\n",
    "    ccf = hdul[0].data           # a Table HDU\n",
    "    allcpp.append(np.max([np.nanpercentile(ccf[od], 95) for od in range(total_order)]))\n",
    "\n",
    "import pdb;pdb.set_trace()\n",
    "reweighting_method = 'ccf_max'\n",
    "index = np.where(allcpp == np.max(allcpp))\n",
    "index = np.squeeze(index[0])\n",
    "print(index, lev2_result_files[index])\n",
    "import pdb;pdb.set_trace()\n",
    "template_file_hdulist = fits.open(lev2_result_files[index])\n",
    "template_ccf = template_file_hdulist[0].data\n",
    "out_csv = MODULE_DIR+'results/NEID/neid_ratio_'+str(s_order)+ '_'+str(e_order)+'.csv'\n",
    "import pdb;pdb.set_trace()\n",
    "rw_ratio_df = RadialVelocityAlg.make_reweighting_ratio_table(template_ccf[0:total_order, :], \n",
    "                                            s_order, e_order, reweighting_method, max_ratio=1, output_csv=out_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RV vs. time from the results of above RV computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_dev_stats = rv_dev_info.analyze_multiple_ccfs()\n",
    "rv_ref_stats = rv_ref_info.analyze_multiple_ccfs()\n",
    "rv_stats = {'developed': rv_dev_stats, 'reference': rv_ref_stats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(rv_stats)\n",
    "plot_velocity_time(rv_stats, method, {dev: 'developed', ref: 'neid'},\n",
    "                   {dev: 'cyan', ref: 'yellow'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RV computation on HARPS L1 data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "harps_L1_dir = TEST_DIR + '/rv_test/HARPStauceti_baseline/e2ds/'\n",
    "harps_L2_dir = TEST_DIR + '/rv_test/HARPStauceti_baseline/ccf/'\n",
    "outfolder = MODULE_DIR+'results/HARPS/'\n",
    "\n",
    "s_order= 0\n",
    "e_order= 69\n",
    "s_x_pos = 500\n",
    "e_x_pos = 3500\n",
    "\n",
    "spectraname_harps = glob.glob(harps_L1_dir + 'HARPS.*_e2ds_A.fits')\n",
    "level2_harps = glob.glob(harps_L2_dir + 'HARPS.*_ccf_G2_A.fits')\n",
    "spectraname_harps.sort()\n",
    "\n",
    "#print('<test>: \\n ', '\\n'.join(spectraname_harps))\n",
    "\n",
    "k_harps = 'HARPS.20'\n",
    "k_e2ds = '_e2ds_A'\n",
    "k_ccf = '_ccf_G2_A'\n",
    "\n",
    "start_idx = spectraname_harps[0].index(k_harps)+len(k_harps)\n",
    "end_idx = spectraname_harps[0].index(k_e2ds)\n",
    "total_file = len(spectraname_harps)\n",
    "\n",
    "code_list = [ f[start_idx:end_idx] for f in spectraname_harps]\n",
    "print('<test>: \\n', 'total ', (len(code_list)), ' code in fits: ', code_list)\n",
    "\n",
    "config_file = MODULE_DIR+'configs/default_harps.cfg'\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file)\n",
    "\n",
    "logger = start_logger(\"OrderTraceAlg\", config_file)\n",
    "\n",
    "rv_init = RadialVelocityAlgInit(config, logger)\n",
    "init_result = rv_init.start(print_debug='')\n",
    "\n",
    "rv_dev_info = RadialVelocityStats()\n",
    "rv_ref_info = RadialVelocityStats()\n",
    "rv_arp_info = RadialVelocityStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rv_guess = init_result['data']['rv_config']['start_rv']\n",
    "for f in range(total_file):\n",
    "    code = code_list[f]\n",
    "    spec_name = spectraname_harps[f]\n",
    "    print('File ', f+1, 'of ', total_file, ' file: ', spec_name, ' code: ', code)\n",
    "    spec_data, spec_header = fits.getdata(spec_name, header=True)\n",
    "    \n",
    "    publicfile = harps_L2_dir+k_harps+code+k_ccf+'.fits'\n",
    "    print('<test> :','harps L2 file: ', publicfile )\n",
    "    harps_ccf, ccf_head = fits.getdata(publicfile, header=True)\n",
    "    \n",
    "    radial_vel = RadialVelocityAlg(spec_data, spec_header, init_result, None, config=config, logger=None)\n",
    "    ref_ccf = harps_ccf[s_order:e_order+1, :]\n",
    "    cindy_rv  = radial_vel.compute_rv_by_cc(start_order = s_order, \n",
    "                                            end_order = e_order, \n",
    "                                            start_x = s_x_pos, \n",
    "                                            end_x = e_x_pos, ref_ccf = ref_ccf, print_progress=\"\")\n",
    "    \n",
    "    cindy_ccf = cindy_rv['ccf_ary']\n",
    "    total_order = radial_vel.spectrum_order\n",
    "    gaussian_fit, gaussian_mean, g_x, g_y = radial_vel.fit_ccf(\n",
    "                            cindy_ccf[total_order+RadialVelocityAlg.ROWS_FOR_ANALYSIS-1, :], rv_guess, \n",
    "                            cindy_ccf[total_order+RadialVelocityAlg.ROWS_FOR_ANALYSIS-2, :])         \n",
    "    new_harps_ccf = harps_ccf.copy()\n",
    "    new_harps_ccf[71, :] = cindy_ccf[71, :]      # copy the velocity step\n",
    "    harps_fit, harps_mean, h_x, h_y = radial_vel.fit_ccf(\n",
    "        new_harps_ccf[total_order+RadialVelocityAlg.ROWS_FOR_ANALYSIS-1, :], rv_guess,\n",
    "        new_harps_ccf[total_order+RadialVelocityAlg.ROWS_FOR_ANALYSIS-2, :])\n",
    "    \n",
    "    harps_mean_head = ccf_head['HIERARCH ESO DRS CCF RVC']\n",
    "    L1_time = cindy_rv['jd'] \n",
    "    print(\"harps mean: \", harps_mean, harps_mean_head, 'jd: ', L1_time, ccf_head['MJD-OBS'])\n",
    "       \n",
    "                   \n",
    "    plot_gaussian_on_curve(gaussian_fit, g_x, g_y, title=spectraname_harps[f],\n",
    "                           ref_curve=h_y, ref_gaussian=harps_fit)\n",
    "    \n",
    "    # compare to the result before porting to make sure the porting produces the same result\n",
    "    \"\"\"\n",
    "    target_file_to_test = '/Users/cwang/documents/KPF/KPF-Pipeline/AlgorithmDev_07122020/test_data_02242020/rv_test'+\\\n",
    "          '/output_cindy_08042020/HARPStaucetiCindy.20'+code+'_norm_ccf_G2_A.fits'\n",
    "          \n",
    "    # target_file_to_test = outfolder+'HARPStaucetiCindy.20'+code+'_norm_ccf_G2_A.fits'   \n",
    "    # import pdb;pdb.set_trace()    \n",
    "    print('taget file: ', target_file_to_test)  \n",
    "    target_data = fits.getdata(target_file_to_test)\n",
    "    compare_result = result_test(target_data, cindy_ccf)\n",
    "    print(compare_result)\n",
    "    \"\"\"\n",
    "    \n",
    "    rv_arpita_fits = '/Users/cwang/documents/KPF/KPF-Pipeline/AlgorithmDev_07122020/test_data_02242020/rv_test/'+\\\n",
    "    '/output_arpita/HARPStaucetiARPITA.20'+code+'_norm_ccf_G2_A.fits'\n",
    "    arpita_ccf, arpita_head = fits.getdata(rv_arpita_fits, header=True)\n",
    "    arpita_fit, arpita_mean, a_x, a_y = radial_vel.fit_ccf(\n",
    "        arpita_ccf[total_order+RadialVelocityAlg.ROWS_FOR_ANALYSIS-1, :], rv_guess,\n",
    "        arpita_ccf[total_order+RadialVelocityAlg.ROWS_FOR_ANALYSIS-2, :])\n",
    "    \n",
    "    # make_fits(cindy_ccf, gaussian_mean, target_file_to_test)\n",
    "        \n",
    "    rv_dev_results, total_dev_rv = rv_dev_info.add_data(gaussian_fit.mean.value, L1_time)\n",
    "    rv_ref_results, total_ref_rv = rv_ref_info.add_data(harps_mean_head, L1_time)    \n",
    "    rv_arp_results, total_arp_rv = rv_arp_info.add_data(arpita_mean, L1_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing data from the module, harps and Arpita's implementation\n",
    "rv_dev_stats = rv_dev_info.analyze_multiple_ccfs()\n",
    "rv_ref_stats = rv_ref_info.analyze_multiple_ccfs()\n",
    "rv_arp_stats = rv_arp_info.analyze_multiple_ccfs()\n",
    "\n",
    "rv_stats = {'dev2': rv_arp_stats, dev: rv_dev_stats, ref: rv_ref_stats}\n",
    "\n",
    "plot_velocity_time(rv_stats, \"RV for harps\", {dev: 'developed', ref: 'harps', 'dev2': 'Arpita'},\n",
    "                   {dev: 'cyan', ref: 'yellow', 'dev2': 'green'}, time_unit='day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
