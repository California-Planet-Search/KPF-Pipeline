{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcbcaa85",
   "metadata": {},
   "source": [
    "# KPF Processing Scripts\n",
    "This page describes the scripts uses for processing KPF data and the ones that are used for production processing.\n",
    "\n",
    "Before listing the scripts, we need to import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0133a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: IERSStaleWarning: leap-second file is expired. [astropy.utils.iers.iers]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All plotting processes have completed.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "codepath = '/code/KPF-Pipeline/scripts/'\n",
    "sys.path.append(codepath)\n",
    "import generate_time_series_plots\n",
    "import ingest_dates_kpf_tsdb\n",
    "import ingest_watch_kpf_tsdb\n",
    "import kpf_processing_progress\n",
    "import launch_kpf_tsdb_plotting\n",
    "import make_plots_kpf_tsdb\n",
    "import qlp_parallel\n",
    "from modules.Utils.string_proc import print_shell_script_docstring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5051d5df",
   "metadata": {},
   "source": [
    "### generate_time_series_plots.py\n",
    "This commandline script is used ingest keywords, RVs, and other information into an observational database that can be used for various purposes including making plots of performance over time.  It is part of the production processing pipeline.  The docstring for this is below. Compared with ingest_watch_kpf_tsdb.py (below), this script is used for a specified date range and is not typically used in production processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b552694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Script Name: generate_time_series_plots.py\n",
      "\n",
      "Description:\n",
      "    This script generates KPF time series plots over various time intervals and\n",
      "    saves the results to predefined directories. It supports multithreaded execution\n",
      "    for tasks with different intervals and date ranges, including daily, monthly,\n",
      "    yearly, and decade-based plots. Additionally, the script monitors the status\n",
      "    of running threads and reports on their activity.\n",
      "\n",
      "Features:\n",
      "    - Generates plots for multiple time intervals (day, month, year, decade).\n",
      "    - Supports custom date ranges for plot generation.\n",
      "    - Multithreaded execution for efficiency, allowing simultaneous tasks.\n",
      "    - Monitors thread status and execution time for each task.\n",
      "    - Saves results in a structured format for further analysis.\n",
      "\n",
      "Usage:\n",
      "    Run this script with optional arguments to specify the database path:\n",
      "\n",
      "        python generate_time_series_plots.py --db_path /path/to/database.db\n",
      "\n",
      "Options:\n",
      "    --db_path   Path to the time series database file. Default: /data/time_series/kpf_ts.db\n",
      "\n",
      "Examples:\n",
      "    1. Using the default database path:\n",
      "        python generate_time_series_plots.py\n",
      "\n",
      "    2. Specifying a custom database path:\n",
      "        python generate_time_series_plots.py --db_path /custom/path/to/kpf_ts.db\n",
      "\n",
      "    3. Monitoring thread statuses:\n",
      "        The script automatically reports on thread activity and execution times every 5 minutes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_time_series_plots.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ca2d7",
   "metadata": {},
   "source": [
    "### ingest_dates_kpf_tsdb.py\n",
    "This commandline script is used ingest keywords, RVs, and other information into an observational database that can be used for various purposes including making plots of performance over time.  It is part of the production processing pipeline.  The docstring for this is below. Compared with ingest_watch_kpf_tsdb.py (below), this script is used for a specified date range and is not typically used in production processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "998428a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Script Name: ingest_dates_kpf_tsdb.py\n",
      "   \n",
      "    Description:\n",
      "      This script is used to ingest KPF observations over a date range into a \n",
      "      KPF Time Series Database.\n",
      "\n",
      "    Options:\n",
      "      --help        Display help message\n",
      "      --start_date  Start date in YYYYMMDD format\n",
      "      --end_date    End date in YYYYMMDD format\n",
      "   \n",
      "    Usage:\n",
      "      ./ingest_dates_kpf_tsdb.py YYYYMMDD YYYYMMDD dbname.db\n",
      "   \n",
      "    Example:\n",
      "      ./ingest_dates_kpf_tsdb.py 20231201 20240101 kpfdb.db\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(ingest_dates_kpf_tsdb.main.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff62a3b9",
   "metadata": {},
   "source": [
    "### ingest_watch_kpf_tsdb.py\n",
    "The commandline script ingest_watch_kpf_tsdb.py is used ingest keywords, RVs, and other information into an observational database that can be used for various purposes including making plots of performance over time.  The docstring for this is below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9970166c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Script Name: ingest_watch_kpf_tsdb.py\n",
      "\n",
      "Description:\n",
      "    This script watches directories for new or modified KPF files and ingests their\n",
      "    data into a KPF Time Series Database. The script utilizes the Watchdog library\n",
      "    to monitor filesystem events and triggers ingestion processes. Additionally, it \n",
      "    performs periodic scans of data directories to ensure all observations are \n",
      "    ingested.\n",
      "\n",
      "Features:\n",
      "    - Ingests file metadata and telemetry into the database.\n",
      "    - Watches multiple directories for new or modified KPF files.\n",
      "    - Performs periodic scans of data directories.\n",
      "    - Supports multithreaded execution.\n",
      "\n",
      "Usage:\n",
      "    Run this script with optional arguments to specify the database path:\n",
      "    \n",
      "        python ingest_watch_kpf_tsdb.py --db_path /path/to/database.db\n",
      "\n",
      "Options:\n",
      "    --db_path   Path to the time series database file. Default: /data/time_series/kpf_ts.db\n",
      "\n",
      "Examples:\n",
      "    1. Using default database path:\n",
      "        python ingest_watch_kpf_tsdb.py\n",
      "\n",
      "    2. Specifying a custom database path:\n",
      "        python ingest_watch_kpf_tsdb.py --db_path /custom/path/to/kpf_ts.db\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ingest_watch_kpf_tsdb.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a68e5d",
   "metadata": {},
   "source": [
    "### kpf_slowtouch.sh\n",
    "Add description.  The docstring is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742a6f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script name: kpf_slowtouch.sh\n",
      "Author: Andrew Howard\n",
      "        with assistance from Chat-GPT4\n",
      "        or maybe the other way around\n",
      "Date: June 23, 2023\n",
      "\n",
      "This script is used to touch a list of KPF L0 files that have names like\n",
      "KP.20230623.12345.67.fits.  This is useful to initiate reprocessing\n",
      "using the KPF DRP.  The list of L0 files can be provided in multiple ways:\n",
      "   1. As command-line arguments when invoking the script.\n",
      "   2. In the first column of a CSV file specified with the -f option.\n",
      "      This is useful for CSV files with a large set of L0 filenames\n",
      "      downloaded from Jump.  Such files might have double quotes around\n",
      "      the L0 filename, which the script will remove when appropriate.\n",
      "   3. All filenames in a directory specified with the -d option.\n",
      "\n",
      "Command-line options (all are optional):\n",
      "-f <filename>       : The script will read the KPF L0 filenames\n",
      "                      from the first column of a CSV with the name <filename>.\n",
      "                      Useful for lists of L0 files downloaded from Jump.\n",
      "-d <directory>      : Adds every file in <directory> to the list of L0 files.\n",
      "-p <path>           : Sets the L0 path to <path>.\n",
      "                      Default value: /data/kpf/L0\n",
      "-s <sleep_interval> : Sets the interval between file touches.\n",
      "                      Default value: 0.2 [sec]\n",
      "-e                  : Echo the touch commands instead of executing them.\n",
      "\n",
      "Examples:\n",
      "1. To provide filenames using command line arguments:\n",
      "   ./kpf_slowtouch.sh KP.20230623.12345.67.fits KP.20230623.12345.68.fits\n",
      "2. To provide filenames using a CSV file:\n",
      "   ./kpf_slowtouch.sh -f filenames.csv\n",
      "3. To provide files listed in a directory:\n",
      "   ./kpf_slowtouch.sh -d /path/to/directory\n",
      "4. To change the default L0 path and sleep interval between touches:\n",
      "   ./kpf_slowtouch.sh KP.20230623.12345.67.fits -p /new/path -s 0.5\n",
      "5. To echo the touch commands instead of executing them:\n",
      "   ./kpf_slowtouch.sh KP.20230623.12345.67.fits -e\n"
     ]
    }
   ],
   "source": [
    "print_shell_script_docstring(codepath + 'kpf_slowtouch.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9728ae56",
   "metadata": {},
   "source": [
    "### kpf_processing_progress.py\n",
    "This commandline script is used to assess the status and progress of processing KPF data.  The docstring is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68b92a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Script Name: kpf_processing_progress.py\n",
      "   \n",
      "    Description:\n",
      "      This script is used to assess the status and progress of processing KPF data.\n",
      "      It searches over a range of dates specified by the first two arguments which are \n",
      "      of the form YYYYMMDD.  For each date (with /data/kpf/L0/YYYYMMDD as the \n",
      "      assumed L0 directory), it examines each L0 file and the associated 2D/L1/L2 \n",
      "      files in their related directories.  If the first argument is a date after the \n",
      "      second argument, then the dates are printed in reverse chronological order (later \n",
      "      dates first).  The output of this script is a table with columns indicating the \n",
      "      date for each row, the most recent modification date for and L0 file in that \n",
      "      directory, the fraction of 2D files processed, the fraction of L1 files processed, \n",
      "      and the fraction of L2 files processed.  Sample output is shown below.\n",
      "      \n",
      "      > ./scripts/kpf_processing_progress.py 20231231 20230101 --current_version 2.5\n",
      "\n",
      "      \n",
      "      DATECODE | LAST L0 MOD DATE | 2D PROCESSING  | L1 PROCESSING  | L2 PROCESSING \n",
      "      ------------------------------------------------------------------------------\n",
      "      20231221 | 2023-12-21 10:18 |  256/256  100% |  254/256   99% |  229/230   99%\n",
      "      20231220 | 2023-12-20 16:00 |  342/342  100% |  342/342  100% |  315/315  100%\n",
      "      20231219 | 2023-12-19 16:00 |  406/406  100% |  406/406  100% |  377/379   99%\n",
      "      20231218 | 2023-12-18 16:00 |  531/531  100% |  528/531   99% |  501/504   99%\n",
      "      20231217 | 2023-12-17 16:00 |  524/524  100% |  524/524  100% |  497/497  100%\n",
      "      20231216 | 2023-12-16 16:00 |  527/527  100% |  524/527   99% |  497/500   99%\n",
      "      \n",
      "      The following criteria are used to determine if 2D/L1/L2 files are \"processed\":\n",
      "      \n",
      "          - not in the junk file list ('/data/kpf/reference/Junk_Observations_for_KPF.csv');\n",
      "            if the file is missing, all files are assumed to not be junk\n",
      "          - have the Green, Red, or CaHK extension present in the L0 file\n",
      "          - not a Dark or Bias exposure [only applied to L2 files]\n",
      "          - the 2D/L1/L2 exists\n",
      "          - the modification time of the 2D/L1/L2 file is later than the \n",
      "            modification time of the associated L0 file\n",
      "          - the DRP version number is equal to or greater than the current DRP version \n",
      "            number of the master branch on Github [only if --check_version option \n",
      "            selected]\n",
      "      \n",
      "                    #    - not junk\n",
      "                    #    - Green, Red, or CaHK extension present\n",
      "                    #    - not a Dark or Bias exposure\n",
      "                    #    - file present\n",
      "                    #    - L2 modification time more recent than L0 modification time\n",
      "                    #    - current DRP version number (if check_version option selected)\n",
      "      \n",
      "      Command-line options listed below enable touching of the L0 files associated \n",
      "      with 2D/L1/L2 files that are not present, printing those filenames, printing the \n",
      "      filenames of the 2D/L1/L2 files themselves, and turning on the DRP version check.\n",
      "\n",
      "    Options:\n",
      "      --help             Display this message\n",
      "      --print_files      Display missing file names (or files that fail other criteria)\n",
      "      --print_files_2D   Display missing 2D file names (or files that fail other criteria)\n",
      "      --print_files_L1   Display missing L1 file names (or files that fail other criteria)\n",
      "      --print_files_L2   Display missing L2 file names (or files that fail other criteria)\n",
      "      --touch_files      Touch the base L0 files of missing 2D/L1/L2 files\n",
      "      --check_version    Checks that each 2D/L1/L2 file has the current Git version for the KPF-Pipeline\n",
      "      --current_version  The current version of determining completion status; e.g. --current version 2.5\n",
      "   \n",
      "    Usage:\n",
      "      kpf_processing_progress.py YYYYMMDD [YYYYMMDD] [--print_files] [--print_files_2D] [--print_files_L1] [--print_files_L2] [--touch_files] [--check_version]\n",
      "   \n",
      "    Example:\n",
      "      ./scripts/kpf_processing_progress.sh 20231114 20231231 --print_files\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(kpf_processing_progress.main.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115ba2d7",
   "metadata": {},
   "source": [
    "### launch_kpf_tsdb_plotting.py\n",
    "This commandline script is used to automatically ingest information from KPF data products into an Observational Database.  It is part of the production processing pipeline.  The docstring is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5af4d168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Script Name: ingest_watch_kpf_tsdb.py\n",
      "\n",
      "Description:\n",
      "    This script watches directories for new or modified KPF files and ingests their\n",
      "    data into a KPF Time Series Database. The script utilizes the Watchdog library\n",
      "    to monitor filesystem events and triggers ingestion processes. Additionally, it \n",
      "    performs periodic scans of data directories to ensure all observations are \n",
      "    ingested.\n",
      "\n",
      "Features:\n",
      "    - Ingests file metadata and telemetry into the database.\n",
      "    - Watches multiple directories for new or modified KPF files.\n",
      "    - Performs periodic scans of data directories.\n",
      "    - Supports multithreaded execution.\n",
      "\n",
      "Usage:\n",
      "    Run this script with optional arguments to specify the database path:\n",
      "    \n",
      "        python ingest_watch_kpf_tsdb.py --db_path /path/to/database.db\n",
      "\n",
      "Options:\n",
      "    --db_path   Path to the time series database file. Default: /data/time_series/kpf_ts.db\n",
      "\n",
      "Examples:\n",
      "    1. Using default database path:\n",
      "        python ingest_watch_kpf_tsdb.py\n",
      "\n",
      "    2. Specifying a custom database path:\n",
      "        python ingest_watch_kpf_tsdb.py --db_path /custom/path/to/kpf_ts.db\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ingest_watch_kpf_tsdb.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dc4453",
   "metadata": {},
   "source": [
    "### launch_qlp.sh\n",
    "This commandline scirpt is used to launch multiple instances of the quicklook pipeline to generate standard diagnostic plots for L0/2D/L1/L2/master data products.  It is part of the production processing pipeline.  The docstring is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2a76909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script name: launch_qlp.sh\n",
      "Author: Andrew Howard\n",
      "\n",
      "This script launches 15 QLP (Quicklook Pipeline) instances for data levels\n",
      "L0, 2D, L1, L2, and masters. It utilizes the specified recipe and config\n",
      "files to process observational data in the KPF pipeline. The script can\n",
      "optionally process only recent observations from the current day.\n",
      "\n",
      "Command-line options (all are optional):\n",
      "  --only_recent       Use a specialized recipe to process only observations\n",
      "                      from the current day.\n",
      "  -h, --help          Display this help message and exit.\n",
      "\n",
      "Examples:\n",
      "1. Launch QLP instances for all data levels with the default recipe:\n",
      "   ./launch_qlp.sh\n",
      "\n",
      "2. Launch QLP instances for only recent observations:\n",
      "   ./launch_qlp.sh --only_recent\n",
      "\n",
      "3. Display the help message:\n",
      "   ./launch_qlp.sh -h\n"
     ]
    }
   ],
   "source": [
    "print_shell_script_docstring(codepath + 'launch_qlp.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdf9bc4",
   "metadata": {},
   "source": [
    "### make_plots_kpf_tsdb.py\n",
    "This commandline script is used to generate standard KPF time series plots of telemetry and other information.  It uses the Observational database and is part of the production processing pipeline.  The docstring is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dfb4598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Script Name: make_plots_kpf_tsdb.py\n",
      "\n",
      "Description:\n",
      "    This script generates standard KPF Time Series plots from the database. \n",
      "    It supports plotting data over specific intervals such as day, month, year, \n",
      "    decade, or custom ranges like the last N days. The plots are saved to \n",
      "    predefined directories for further use or analysis.\n",
      "\n",
      "Features:\n",
      "    - Creates time series plots for various intervals (day, month, year, decade).\n",
      "    - Supports custom ranges like \"last N days\".\n",
      "    - Saves plots in a structured directory format.\n",
      "    - Includes configurable wait times for process orchestration.\n",
      "\n",
      "Usage:\n",
      "    Run this script with required arguments to specify the database path, \n",
      "    interval, and wait time:\n",
      "\n",
      "        python make_plots_kpf_tsdb.py --db_path /path/to/database.db --interval <interval> --wait_time <seconds>\n",
      "\n",
      "Options:\n",
      "    --db_path       Path to the time series database file. \n",
      "                    Default: /data/time_series/kpf_ts.db\n",
      "    --interval      Interval for plotting. Supported values:\n",
      "                    - 'day', 'month', 'year', 'decade'\n",
      "                    - 'last_<N>_days' (e.g., 'last_7_days' for the last 7 days)\n",
      "    --wait_time     Time (in seconds) to wait before exiting the script.\n",
      "\n",
      "Examples:\n",
      "    1. Generate plots for the current year:\n",
      "        python make_plots_kpf_tsdb.py --interval year --wait_time 10\n",
      "\n",
      "    2. Generate plots for the last 30 days:\n",
      "        python make_plots_kpf_tsdb.py --interval last_30_days --wait_time 10\n",
      "\n",
      "    3. Generate daily plots and specify a custom database path:\n",
      "        python make_plots_kpf_tsdb.py --db_path /custom/path/to/kpf_ts.db --interval day --wait_time 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(make_plots_kpf_tsdb.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272b3a28",
   "metadata": {},
   "source": [
    "### qlp_parallel.py\n",
    "This commandline script is used to reprocess Quicklook data products over a specified date range.  The docstring is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "659b6c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Script Name: qlp_parallel.py\n",
      "   \n",
      "    Description:\n",
      "      This commandn line script uses the 'parallel' utility to execute the recipe \n",
      "      called 'recipes/quicklook_match.recipe' to generate standard Quicklook data \n",
      "      products.  The script selects all KPF files based on their\n",
      "      type (L0/2D/L1/L2/master) from the standard data directory using a date \n",
      "      range specified by the parameters start_date and end_date.  L0 files are \n",
      "      included if the --l0 flag is set or none of the --l0, --2d, --l1, --l2\n",
      "      flags are set (in which case all data types are included).  The --2d, \n",
      "      --l1, and --l2 flags have similar functions.  The script assumes that it\n",
      "      is being run in Docker and will return with an error message if not. \n",
      "      If start_date is later than end_date, the arguments will be reversed \n",
      "      and the files with later dates will be processed first.\n",
      "      \n",
      "      Invoking the --print_files flag causes the script to print filenames\n",
      "      but not create QLP data products.\n",
      "      \n",
      "      The --ncpu parameter determines the maximum number of cores used.  \n",
      "      \n",
      "      The following feature is not operational if this script is run inside of \n",
      "      a Docker container: If the --load parameter (a percentage, e.g. 90 = 90%) \n",
      "      is set to a non-zero value, this script will be throttled so that no new \n",
      "      files will have QLPs processed until the load is below that value.  Note \n",
      "      that throttling works in steady state; it is possible to overload the \n",
      "      system with the first set of jobs if --ncpu is set too way high.  \n",
      "\n",
      "    Arguments:\n",
      "      start_date     Start date as YYYYMMDD, YYYYMMDD.SSSSS, or YYYYMMDD.SSSSS.SS\n",
      "      end_date       End date as YYYYMMDD, YYYYMMDD.SSSSS, or YYYYMMDD.SSSSS.SS\n",
      "\n",
      "    Options:\n",
      "      --l0           Select all L0 files in date range\n",
      "      --2d           Select all 2D files in date range\n",
      "      --l1           Select all L1 files in date range\n",
      "      --l2           Select all L2 files in date range\n",
      "      --master       Select all master files in date range\n",
      "      --ncpu         Number of cores used for parallel processing; default=10\n",
      "      --load         Maximum load (1 min average); default=0 (only activated if !=0)\n",
      "      --print_files  Display file names matching criteria, but don't generate Quicklook plots\n",
      "      --help         Display this message\n",
      "   \n",
      "    Usage:\n",
      "      python qlp_parallel.py YYYYMMDD.SSSSS YYYYMMDD.SSSSS --ncpu NCPU --load LOAD --l0 --2d --l1 --l2 --master --print_files\n",
      "    \n",
      "    Examples:\n",
      "      ./scripts/qlp_parallel.py 20230101.12345.67 20230101.17 --ncpu 50 --l0 --2d\n",
      "      ./scripts/qlp_parallel.py 20240501 20240505 --ncpu 150 --load 90\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(qlp_parallel.main.__doc__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
